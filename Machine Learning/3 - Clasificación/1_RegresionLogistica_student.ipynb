{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8CiTgix/MuabaUPsPk5MB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Modelos de Clasificación\n","\n","------------------------------------------------------\n","\n","\n","### Data Science and Machine Learning\n","\n","#### Marzo 2023\n","\n","**Aurora Cobo Aguilera**\n","\n","**The Valley**\n","\n","------------------------------------------------------\n","\n","\n","\n","En este notebook hay mucho contenido teórico matemático. Te lo puedes saltar si te resulta difícil seguirlo. Tómalo como un material extra! :)"],"metadata":{"id":"SzdZ9OS-Jsh5"}},{"cell_type":"markdown","source":["Los modelos de clasificación también se engloban dentro del aprendizaje supervisado. En este caso el *target* es una **clase** de entre un conjunto de clases predefinidas a priori. En el caso de que las clases sean solo 2, hablamos de **clasificación binaria** y estas clases se suelen denominar **positiva** ($y=+1$) y **negativa** ($y=-1$), o 1 y 0. En caso de que haya más de dos clases hablamos de escenarios **multiclase**.\n","\n","Probablemente los problemas de clasificación automática son los más estudiados dentro del mundo del aprendizaje automático. Alguno de los algoritmos más referenciados son:\n","\n","- k Vecinos más próximos (kNN)\n","- Árboles de decisión\n","- Random Forests\n","- Regresión logística\n","- Clasificador lineal de Fisher\n","- Máquina de Vectores Soporte (SVM)\n"],"metadata":{"id":"cqTYkz2UMOfm"}},{"cell_type":"markdown","source":["En un problema de clasificación genérico, nos dan un vector de observaciones \n"," ${\\bf x}\\in \\mathbb{R}^D$ que pertenece a una y sólo una *categoría* o *clase*, $y$, en el conjunto $Y = \\{0, 1, \\ldots, C-1\\}$. El objetivo de un clasificador es predecir el valor de $y$ basado en ${\\bf x}$.\n","\n","Para diseñar el clasificador, nos dan una colección de observaciones etiquetadas $D = \\{({\\bf x}^{(n)}, y^{(n)})\\}_{n=1}^N$ donde, para cada observación ${\\bf x}^{(n)}$, el valor de su categoría real, $y^{(n)}$, es conocido.\n","\n","------\n","\n","En este notebook estudiaremos el clasificador llamado regresión logística, por su parecido a una regresión lineal, con la particular diferencia de aplicar una función que nos devuelva probabilidades para cada clase del problema a resolver. Para ello, antes estudiaremos y veremos un poco la base teórica en la que se fundamenta dicho modelo y aplicaremos los conceptos de probabilidad vistos en teoría. No hace falta que entendáis a la perfeción todas las ecuaciones y la teoría, pero así podéis ver la relación de la teoría de la probabilidad en la creación de modelos de aprendizaje automático.\n","\n","Además, veremos cómo podemos incluir no linealidades a este modelo, estudiaremos técnicas de regularización para no sobreajustar y por último, definiremos y utilizaremos en un ejemplo práctico, diferentes medidas de evaluar un problema de clasificación."],"metadata":{"id":"6RoV6CxUJsAE"}},{"cell_type":"markdown","metadata":{"id":"L8sceujGKKwS"},"source":["### 0.1. Clasificación binaria\n","\n","Primero nos centraremos en un problema de clasificación binaria, donde el conjunto de etiquetas es binario $Y = \\{0, 1\\}$ y las observaciones $X\\in \\mathbb{R}^D$. A pesar de su simplicidad, es el caso más frecuente. Muchos problemas de clasificación multi-clase se resuelven descomponiéndolos en colecciones de problemas binarios."]},{"cell_type":"markdown","metadata":{"id":"bRA28xLNKKwS"},"source":["### 0.2. La suposición i.i.d.\n","\n","En algoritmos de clasificación, como en muchos otros en ML, se basan en dos hipótesis principales:\n","\n","   - Todas las muestras en el dataset $D$ son i.i.d. (independientes e indénticamente distribuidas), i.e., todas la muestras son salidas independientes de una distribución desconocida $p({\\bf x}, y)$.\n","   - Para cualquier dato de test, la tupla formada por la muestra de entrada y su clase desconocida, $({\\bf x}, y)$, es una salida independiente de la *misma* distribución.\n","   \n","Estas dos suposiciones son esenciales para tener garantías de que el clasificador diseñado basado en $D$ tenga buenos resultados en nuestras muestras de test. Ten en cuenta que a pesar de asumir la presencia de una distribución subyacente, ésta es desconocida: en otro caso, podríamos ignorar $D$ y aplicar teoría de la decisión clásica para encontrar el predictor óptimo basado en $p({\\bf x}, y)$.\n","\n","### 0.3. Clasificación binaria y teoría de la decisión. El criterio MAP \n","\n","El objetivo de un problema de clasificación binaria es asignar una *clase* o *categoría* a cada *entrada* u *observación* de una colección de datos. Aquí asumiremos que cada entrada ${\\bf x}$ es un vector $D$-dimensional en $\\mathbb{R}^D$, y que la clase $y$ de la muestra ${\\bf x}$ es un elemento de un conjunto binario $Y = \\{0, 1\\}$. El objetivo del clasificador es predecir el valor verdadero de $y$ después de observar ${\\bf x}$.\n","\n","Definiremos como $\\hat{y}$ la salida del clasificador o *decisión*. Si $y=\\hat{y}$, la decisión es un *acierto*, en otro caso $y\\neq \\hat{y}$ y la decisión es un *error*.\n","\n","La **Teoría de la Decisión** proporciona una solución al problema de clasificación en situaciones donde la relación entre las entradas ${\\bf x}$ y su clase $y$ es dada por un modelo probabilístico conocido: Asume que cada tupla $({\\bf x}, y)$ es una salida de un vector aleatorio $({\\bf X}, Y)$ con distribución conjunta $p_{{\\bf X},Y}({\\bf x}, y)$. Un criterio natural para clasificación es seleccionar el predictor $\\hat{Y}=f({\\bf x})$ de manera que la probabilidad de error, $P\\{\\hat{Y} \\neq Y\\}$ sea mínima. Teniendo en cuenta que\n","\n","$$\n","P\\{\\hat{Y} \\neq Y\\} = \\int P\\{\\hat{Y} \\neq Y | {\\bf x}\\} p_{\\bf X}({\\bf x}) d{\\bf x}\n","$$\n","\n","la decisión óptima se obtiene si, para cada muestra ${\\bf x}$, tomamos una decisión minimizando la probabilidad de error condicional:\n","\n","\\begin{align}\n","\\hat{y}^* &= \\arg\\min_{\\hat{y}} P\\{\\hat{y} \\neq Y |{\\bf x}\\} \\\\\n","          &= \\arg\\max_{\\hat{y}} P\\{\\hat{y} = Y |{\\bf x}\\} \\\\\n","\\end{align}\n","\n","\n","Entonces, la regla de la decisión óptima se puede expresar como\n","\n","$$\n","P_{Y|{\\bf X}}(1|{\\bf x}) \\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0}\\quad  P_{Y|{\\bf X}}(0|{\\bf x}) \n","$$\n","\n","o, equivalentemente\n","\n","$$\n","P_{Y|{\\bf X}}(1|{\\bf x}) \\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0}\\quad  \\frac{1}{2} \n","$$\n","\n","El clasificador que implementa esta regla de decisión usualmente se llama MAP (*Maximum A Posteriori*). "]},{"cell_type":"markdown","source":["### 0.4. Clasificación paramétrica.\n","\n","La Teoría de la Decisión clásica se basa en la suposición de que el modelo probabilístico relacionando la muestra observada ${\\bf X}$ y la hipótesis real $Y$ es conocida. Desafortunadamente, esto no es realista en muchas aplicaciones, donde la única información disponible para construir el clasificador es un dataset $D = \\{({\\bf x}^{(n)}, y^{(n)}), \\,n=1,\\ldots,N\\}$ de entradas y sus respectivas etiquetas de clase.\n","\n","Una formulación más realista del problema de clasificación es la siguiente: dado un dataset $D = \\{({\\bf x}^{(n)}, y^{(n)}) \\in {\\mathbb{R}}^D \\times Y, \\, n=1,\\ldots,N\\}$ de muestras i.i.d. de una distribución ***desconocida*** $p_{{\\bf X},Y}({\\bf x}, y)$, predice la clase $y$ de una nueva muestra ${\\bf x}$ con la mínima probabilidad de error.\n","\n","Ya que el modelo de probabilidad de generar los datos es desconocido, la regla de decisión MAP no se puede aplicar. Sin embargo, muchos algoritmos de clasificación usan el dataset para obtener estimaciones de las probabilidades de clase a posteriori, y lo aplican para implementar una aproximación del decisor MAP.\n","\n","Los clasificadores paramétricos que se basan en esta idea asumen, adicionalmente, que la probabilidad de clase a posteriori satisface alguna fórmula paramétrica:\n","\n","$$\n","P_{Y|X}(1|{\\bf x},{\\bf w}) = f_{\\bf w}({\\bf x})\n","$$\n","\n","donde ${\\bf w}$ es el vector de parámetros. Dada la expresión del decisor MAP, la clasificación consiste en comparar el valor de $f_{\\bf w}({\\bf x})$ con el umbral $\\frac{1}{2}$, y cada vector de parámetros se asociará a un decisor diferente.\n","\n","En práctica, el dataset $D$ se usa para seleccionar un vector de parámetros particular $\\hat{\\bf w}$ de acuerdo a cierto criterio. Así la regla de decisión será \n","\n","$$\n","f_{\\hat{\\bf w}}({\\bf x}) \\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0}\\quad  \\frac{1}{2} \n","$$\n","\n","En este notebook, exploraremos uno de los modelos más populares basados en clasificación paramétrica: **logistic regression**.\n"],"metadata":{"id":"yBfQOUTWzeDI"}},{"cell_type":"markdown","metadata":{"id":"round-philip"},"source":["## 1. Regresión Logística\n","---\n","\n","\n","La **regresión logística** resuelve un problema de clasificación **binario** aprendiendo un regresor lineal que trata de ajustar la **probabilidad** de que cada una de las observaciones pertenezca a la **clase positiva**. Como ya hemos mencionado, se trata de un modelo paramétrico.\n","\n","### ¿Regresión lineal para clasificar?\n","\n","En regresión lineal, en la que el target $y$ es un valor real ($y\\in\\mathbb{R}$), utilizamos un modelo del tipo\n","\n","$$y \\approx \\mathbf{w}^T \\mathbf{x}$$\n","\n","- Sin embargo, en **clasificación binaria**, el target solo tiene dos posibles valores (que típicamente codificamos como $y\\in\\{0,1\\}$). \n","- En este sentido el modelo de regresión lineal **no puede usarse como tal**. No se adapta de forma sencilla a estos dos únicos valores del target $y$.\n","\n","\n","\n","En regresión logística aproximamos la probabilidad de la etiqueta $y$ usando una **combinación lineal** de las entradas $\\bf x$:\n","\n","$$P(y=1| {\\bf x}) =  \\frac{1}{1+e^{-({\\bf w}^T {\\bf x}+w_0)}}=\\sigma({\\bf w}^T {\\bf x}+w_0)$$\n","$$$$\n","$$ P(y=0| {\\bf x}) = 1- P(Y=y| {\\bf x}) = 1 - \\sigma({\\bf w}^T {\\bf x}+w_0)$$\n","\n","donde \n","\n","- $w_0$ y ${\\bf w} = [w_1, \\ldots, w_D]$ son parámetros o coeficientes que aprenderemos usando el conjunto de datos de entrenamiento $(\\mathbf{x}^{(i)},y^{(i)})_{i=1,\\ldots,N}$.\n","$$$$\n","- $\\sigma(a) = \\frac{1}{1+\\exp(-a)}$ es la función sigmoide o logística.\n","$$$$\n","- Fijaos que la probabilidad $P(y=1| {\\bf x})$ es la misma todos los puntos $\\mathbf{x}$ tales que ${\\bf w}^T {\\bf x}$ es constante. \n","$$$$\n","- Además, para todo $a\\in\\mathbb{R}$, ${\\bf w}^T {\\bf x} + w_0= a$ es la ecuación de un **plano** en un espacio de dimensión D (el número de componentes de $\\mathbf{x}$). Por ejemplo, una recta en el caso $D=2$.\n","\n","Hablemos primero de $\\sigma(x)$.\n","\n","### 1.1. La función sigmoide o logística\n","---\n","\n","La función *sigmoide* se define por\n","\n","$$\\sigma(t) = \\frac{1}{1+e^{-t}}$$\n","\n","La función sigmoide tiene las siguientes propiedades:\n","\n","- **P1**: Salida probabilística: $\\quad 0 \\le \\sigma(t) \\le 1$\n","- **P2**: Simetría: $\\quad \\sigma(-t) = 1-\\sigma(t)$\n","- **P3**: Monótona: $\\quad \\sigma'(t) = \\sigma(t)·[1-\\sigma(t)] \\ge 0$\n","\n","A continuación, definimos la función sigmoide en python, y la representamos gráficamente.\n","\n","\n"]},{"cell_type":"code","source":["# Para visualizar gráficas en notebooks\n","%matplotlib inline\n","\n","# Librerías\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from scipy.stats import multivariate_normal, norm\n","from sklearn.preprocessing import PolynomialFeatures\n","from sklearn import linear_model\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.linear_model import LogisticRegression"],"metadata":{"id":"ojUasBt-8cX2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** completa el código para calcular la función logística según la definición dada anteriormente."],"metadata":{"id":"LajNCjeQRVqU"}},{"cell_type":"code","source":["def logistic(x):                                        \n","   p = #<SOL>\n","   return p\n","\n","x = np.arange(-5,5,1e-3)\n","\n","y = logistic(x)\n","\n","fig = plt.figure()\n","ax = fig.add_subplot(111)\n","\n","ax.plot(x,y,'-b',lw=2)\n","ax.plot(0,0.5,'or',ms=10,label=r'$P(y=1|\\mathbf{x})=0.5$')\n","plt.xlabel('$\\mathbf{w}^T\\mathbf{x}+w_0$')\n","plt.ylabel('$P(y=1|\\mathbf{x})$')\n","plt.legend()\n","# Lineas principales de la rejilla\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","# Lineas menores de la rejilla\n","plt.minorticks_on()\n","plt.grid(visible=True, which='minor', color='gray', alpha=0.2, ls='dotted', lw=1)\n","\n","ax.text(6, 0.5, 'Todos los puntos x tales que $\\mathbf{w}^T\\mathbf{x}+w_0=0$ forman la frontera de decisión.', fontsize=15)\n","\n","plt.show()"],"metadata":{"id":"q7qK3M4L75b6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.2. Clasificador lineal basado en la función logística.\n","---\n","\n","El clasificador MAP bajo la función logística tiene la forma\n","\n","$$P_{Y|{\\bf X}}(1|{\\bf x}, {\\bf w}) = \\sigma({\\bf w}^\\intercal{\\bf x}) \\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0} \\quad \\frac{1}{2} $$\n","\n","Por tanto\n","\n","$$\n","2 \\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0} \\quad  \n","1 + e^{(-{\\bf w}^\\intercal{\\bf x})} $$\n","\n","que es equivalente a\n","\n","$${\\bf w}^\\intercal{\\bf x} \n","\\quad\\mathop{\\gtrless}^{\\hat{y}=1}_{\\hat{y}=0}\\quad \n","0 $$\n","\n","Es decir, los clasificadores basados en la función logística tienen fronteras de decisión lineales pasando por el origen,  ${\\bf x} = {\\bf 0}$. "],"metadata":{"id":"zSUFYyWh-a61"}},{"cell_type":"code","source":["# Vector de pesos\n","w = [1, 4, 8]   # Prueba diferentes pesos\n","\n","# Crea una rejilla rectangular\n","x_min = -1\n","x_max = 1\n","dx = x_max - x_min\n","h = float(dx) / 200\n","xgrid = np.arange(x_min, x_max, h)\n","xx0, xx1 = np.meshgrid(xgrid, xgrid)\n","\n","# Calcula el mapa de la función logística para los pesos dados\n","Z = logistic(w[0] + w[1]*xx0 + w[2]*xx1)\n","\n","# Dibuja el mapa de la función logística\n","fig = plt.figure()\n","ax = fig.add_subplot(projection='3d')\n","ax.plot_surface(xx0, xx1, Z, cmap=plt.cm.copper)\n","plt.xlabel('$x_0$')\n","plt.ylabel('$x_1$')\n","ax.set_zlabel('P(1|x,w)')"],"metadata":{"id":"6kA2IeHJ_USZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.3. Clasificadores no lineales.\n","---\n","\n","Al igual que en el modelo de regresión lineal podíamos incluir no linealidades con transformaciones de las variables de entrada, en la regresión logística podemos proceder de forma análoga.\n","\n","El modleo logístico se puede extender para construir un clasificador no lineal usando transformaciones no lineales de los datos. Una forma de definir el modelo de regresión logística no lineal es:\n","\n","$$P_{Y|{\\bf X}}(1|{\\bf x}, {\\bf w}) = \\sigma([{\\bf w}^\\intercal{\\bf z}({\\bf x})]) $$\n","\n","donde ${\\bf z}({\\bf x})$ es una transformación no lineal arbitraria de las variables originales. La frontera de decisión en este caso es dada por:\n","\n","$$\n","{\\bf w}^\\intercal{\\bf z} = 0\n","$$"],"metadata":{"id":"GbMTykQc_ek3"}},{"cell_type":"markdown","source":["> **Ejercicio**: Modifica el código de arriba para generar una superficie 3D del modelo de regresión logística polinómico dado por: \n","\n","$$\n","P_{Y|{\\bf X}}(1|{\\bf x}, {\\bf w}) = \\sigma(1 + 10 x_0 + 10 x_1 - 20 x_0^2 + 5 x_0 x_1 -20 x_1^2) \n","$$"],"metadata":{"id":"ZksBn17rAwad"}},{"cell_type":"code","source":["#<SOL>\n","\n","#</SOL>"],"metadata":{"id":"EvZsuyb0AwBZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.4 Entendamos la frontera de decisión\n","---\n","\n","En un dataset de dos dimensiones y volviendo al caso lineal, vamos a ver ahora qué ocurre a medida que variamos $\\mathbf{w}$ y $w_0$.\n"],"metadata":{"id":"_3H4S4gMKZhz"}},{"cell_type":"code","source":["data_ejemplo1 = pd.read_csv('http://www.tsc.uc3m.es/~olmos/BBVA/ejemplo1.txt', header=None)\n","\n","data_ejemplo1.head(10)\n"],"metadata":{"id":"5VMv3U3pKYoI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = np.array(data_ejemplo1)\n","\n","## Dimensiones\n","dims = np.shape(data)\n","N = dims[0]\n","\n","## Separamos X e Y\n","X01 = data[:, 0:2]\n","Y1 = data[:, 2]\n","\n","# Separamos train de test\n","X01_train, X01_test, Y1_train, Y1_test = train_test_split(X01, Y1, test_size=0.2, random_state=0)\n","\n","print(\"El conjunto de datos de entrenamiento consta {0:d} observaciones de {1:d} dimensiones\\n\".format(X01_train.shape[0], X01_train.shape[1]))\n","\n","# Variables para la representación de la frontera de decisión (antes de normalizar!)\n","min1 = np.min(X01_train[:,0])-1\n","max1 = np.max(X01_train[:,0])+1\n","min2 = np.min(X01_train[:,1])-1\n","max2 = np.max(X01_train[:,1])+1\n","\n","# Normalizamos los datos (media 0, varianza 1)\n","transformer1 = StandardScaler().fit(X01_train)   # X0 -->sin normalizar, X --> normalizadas\n","\n","X1_train = transformer1.transform(X01_train)\n","X1_test = transformer1.transform(X01_test)"],"metadata":{"id":"Nt95m7coLxjX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## Dibujamos los datos de entrenamiento\n","index = (Y1_train==1)\n","plt.figure()\n","plt.plot(X01_train[index,0],X01_train[index,1],'s',label=\"Class 1\")\n","index=(Y1_train==0)\n","plt.plot(X01_train[index,0],X01_train[index,1],'ro',label=\"Class 0\")\n","plt.xlabel('$x_0$')\n","plt.ylabel('$x_1$')\n","plt.grid(True)\n","plt.legend(loc='upper right')\n","plt.title('Datos de entrenamiento')\n","# Principales líneas de la rejilla\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.show()\n","\n","\n","## Dibujamos los datos de entrenamiento\n","index = (Y1_train==1)\n","plt.figure()\n","plt.plot(X1_train[index,0],X1_train[index,1],'s',label=\"Class 1\")\n","index=(Y1_train==0)\n","plt.plot(X1_train[index,0],X1_train[index,1],'ro',label=\"Class 0\")\n","plt.xlabel('$x_0$')\n","plt.ylabel('$x_1$')\n","plt.ylim([-2,2])\n","plt.grid(True)\n","plt.legend(loc='upper right')\n","plt.title('Datos de entrenamiento (normalizados)')\n","# Principales líneas de la rejilla\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.show()\n","\n"],"metadata":{"id":"USw7hq71MREr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Con el siguiente código representamos las curvas de nivel de igual probabilidad estimadas por RL para distintos vectores $\\mathbf{w}$ fijando $w_0=0$:"],"metadata":{"id":"YjCM9smxMgnu"}},{"cell_type":"code","source":["# Posibles vectores de parámetros\n","v_w = np.array([[1,1],\n","                [1,2.5],\n","                [0.5,-2],\n","                [2,.3]])\n","\n","w0 = 0\n","\n","n_w = v_w.shape[0]\n","\n","# Número de bins para el histograma\n","nbins = 6\n","\n","# Obtenemos una rejilla de puntos en los que evaluaremos nuestro RL \n","# LA REJILLA SE DEFINE EN EL ESPACIO ORIGINAL DE LOS DATOS (ANTES DE NORMALIZAR)\n","x1,x2 = np.mgrid[min1:max1:(max1-min1)/50, min2:max2:(max2-min2)/50]\n","grid = np.transpose(np.row_stack([x1.ravel(), x2.ravel()]))\n","# Normalizamos la rejilla\n","grid_norm = transformer1.transform(grid)\n","\n","values = [0.01] + list(np.arange(0.1,1,0.1)) + [0.99] \n","\n","print(r\"Cada línea representa puntos del plano (x_1,x_2) con igual probabilidad P(y=1|x)\")\n","\n","for iw in range(n_w):\n","    \n","    fx, ax = plt.subplots()\n","    \n","    w = v_w[iw,:]\n","    # Calculamos P(Y=1|X) para todos los puntos de train\n","    z = 1./(1+np.exp(-X1_train.dot(w)-w0))\n","    # Histograma de las probabilidades\n","    hist, bin_edges = np.histogram(z, nbins)\n","    # Calculamos P(Y=1|X) para todos los puntos de la rejilla\n","    zgrid = 1./(1+np.exp(-grid_norm.dot(w)-w0))\n","    \n","    index=(Y1_train==1)\n","    ax.plot(X01_train[index,0],X01_train[index,1],'s',label=\"Class 1\")\n","    index=(Y1_train==0)\n","    ax.plot(X01_train[index,0],X01_train[index,1],'ro',label=\"Class 0\")\n","    ax.set_xlabel(r'$x_0$')\n","    ax.set_ylabel(r'$x_1$')\n","    ax.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","    ax.set_title(\"$w=[{0:.2f}, {1:.2f}]$\".format(w[0],w[1]))\n","    CS = ax.contour(x1,x2,np.reshape(zgrid,np.shape(x1)),values,cmap=plt.cm.gray,linestyles='dashed')\n","    ax.clabel(CS, inline=1, fontsize=17)\n","    ax.legend(loc='upper right')"],"metadata":{"id":"4R_OzEmNMoiP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.5. Inferencia\n","---\n","\n","#### 1.5.1. En pocas palabras: optimizar una función de coste\n","\n","Para seleccionar el vector $\\mathbf{w}$, RL resuelve el siguiente problema de optimización:\n","\n","$$l({\\bf w}) = \\sum_{i=1}^N - \\left\\lbrace  y^{(i)}\\log \\left(  P(Y=1| {\\bf x}^{(i)})\\right)  + (1-y^{(i)}) \\log\\left(  1- P(Y=1| {\\bf x}^{(i)}) \\right)  \\right\\rbrace $$\n","\n","$${\\bf w}^* =\\displaystyle \\underset{{\\bf w}}{\\operatorname{min}} l({\\bf w})$$\n","\n","- Que no es más que intentar alinear las regiones de probabilidades constantes con **las probabilidades de las etiquetas reales**.\n","- Este problema no tiene solución cerrada. Se resuelve con optimización numérica.\n","\n","--- \n","\n","#### 1.5.2. La teoría de probabilidad y cálculo que hay detrás\n","\n","\n","Recuerda que la idea de la clasificación paramétrica es usar las muestras de entrenamiento $D = \\{({\\bf x}^{(n)}, y^{(n)}) \\in {\\mathbb{R}}^D \\times \\{0,1\\}, n=1,\\ldots,N\\}$ para ajustar el vector de los parámetros ${\\bf w}$ de acuerdo a cierto criterio. Entonces, la estimación $\\hat{\\bf w}$ se puede usar para calcular la etiqueta de predicción de cualquier observación como \n","\n","$$\\hat{y} = \\arg\\max_y P_{Y|{\\bf X}}(y|{\\bf x},\\hat{\\bf w}).$$\n","\n","Necesitamos aún elegir un criterio para optimizar con la selección del vector de parámetros. Dos enfoques diferentes para la estimación de ${\\bf w}$ son:\n","\n","   * Maximum Likelihood (ML): $\\hat{\\bf w}_{\\text{ML}} = \\arg\\max_{\\bf w} P_{D|{\\bf W}}(D|{\\bf w})$\n","   * Maximum *A Posteriori* (MAP): $\\hat{\\bf w}_{\\text{MAP}} = \\arg\\max_{\\bf w} p_{{\\bf W}|D}({\\bf w}|D)$"],"metadata":{"id":"3svWtEGcBNQR"}},{"cell_type":"markdown","source":["**Estimación ML**\n","\n","La estimación ML se define como\n","\n","$$\\hat{\\bf w}_{\\text{ML}} = \\arg\\max_{\\bf w} P_{D|{\\bf W}}(D|{\\bf w})\n","   = \\arg\\min_{\\bf w} L({\\bf w})\n","$$\n","\n","donde $L({\\bf w})$ es la función **negative log-likelihood**, dada por\n","\n","$$\n","L({\\bf w}) = - \\log P_{D|{\\bf W}}(D|{\\bf w})\n"," = - \\log\\left[P\\left(y^{(1)},\\ldots,y^{(N)}|\n","     {\\bf x}^{(1)},\\ldots, {\\bf x}^{(N)},{\\bf w}\\right)\\right]\n","$$\n","\n","Usando una serie de suposiciones y cálculo matemático, podemos llegar a la siguiente formulación:\n","\n","\\begin{align}\n","L({\\bf w}) = \\sum_{n=1}^N\\log\\left[1+e^{-\\overline{y}^{(n)}{\\bf w}^\\intercal {\\bf z}^{(n)}}\\right]\n","\\end{align}\n","\n","donde ${\\bf z}^{(n)}={\\bf z}({\\bf x}^{(n)})$.\n","\n","Se puede demostrar además que $L({\\bf w})$ es una función y convexa diferenciable en ${\\bf w}$. Por lo tanto, su mínimo es un punto con gradiente nuelo.\n","\n","\\begin{align}\n","\\nabla_{\\bf w} L(\\hat{\\bf w}_{\\text{ML}}) = - \\sum_{n=1}^N \n","       \\frac{e^{-\\overline{y}^{(n)}\\hat{\\bf w}_{\\text{ML}}^\\intercal {\\bf z}^{(n)}} \\overline{y}^{(n)} {\\bf z}^{(n)}}\n","       {1+e^{-\\overline{y}^{(n)}\\hat{\\bf w}_{\\text{ML}}^\\intercal {\\bf z}^{(n)}\n","       }} = 0\n","\\end{align}\n","\n","Desafortunadamente, $\\hat{\\bf w}_{\\text{ML}}$ no se puede sacar de la ecuación y es necesario aplicar algún modelo iterativo de optimización para buscar el mínimo de la función *negative log-likelihood*. \n"],"metadata":{"id":"uTrnDnkqGXFS"}},{"cell_type":"markdown","source":["**Descenso por gradiente.**\n","\n","Un simple algoritmo de optimización iterative es el <a href = https://en.wikipedia.org/wiki/Gradient_descent> descenso por gradiente</a>. \n","\n","\\begin{align}\n","{\\bf w}_{i+1} = {\\bf w}_i - \\rho_i \\nabla_{\\bf w} L({\\bf w}_i)\n","\\end{align}\n","\n","donde $\\rho_i >0$ es el *learning step*.\n","\n","Aplicando la regla del descenso por gradiente a la regresión logística, obtenemos el siguiente algoritmo:\n","\n","\\begin{align}\n","{\\bf w}_{i+1} &= {\\bf w}_i \n","    + \\rho_i \\sum_{n=1}^N \\left[y^{(n)}-\\sigma({\\bf w}_i^\\intercal {\\bf z}^{(n)})\\right] {\\bf z}^{(n)}\n","\\end{align}"],"metadata":{"id":"Du27pOw5JotH"}},{"cell_type":"markdown","source":["----\n","Con todos estos conceptos nuevos, que sé que en principio pueden parecer muy complicados, sólo quiero que te quedes un poco con el proceso en general, con la aplicación de la teoría de la probabilidad y también con la necesidad de modelos de optimización. Así, a la hora de usar un modelo de scikit-learn, sepas qué significan algunos de los hiperparámetros más usuales, como el *learning rate*, y puedas ajustarlos de la mejor manera posible."],"metadata":{"id":"0nyeA_1IKH-D"}},{"cell_type":"markdown","source":["### 1.6. Ejemplo de una regresión logística lineal\n","\n","En primer lugar, vamos a ver un ejemplo de una regresión logística lineal, es decir sin aplicar ninguna trasnformación $\\mathbf{z}(\\mathbf{x})$ a las variables de entrada. Para ello usaremos la librería de Scikit-learn, por lo que no es necesario programar ninguna de las ecuaciones vistas anteriormente, pero sí darle valor a algún hiperparámetro como sería el *learning rate*.\n","\n","Para ello, vamos a continuar con el mismo ejemplo de datos anterior, pero procederemos a entrenar una regresión logística con los parámetros por defecto en sklearn (luego hablaremos del entrenamiento en sí  y qué parámetros nos ofrece la [implementación de sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html))."],"metadata":{"id":"mI2BwPxPLUw7"}},{"cell_type":"code","source":["mi_RL1 = LogisticRegression(C=1e8)  # C es el parámetro de regularización. Un valor muy alto significa que no regularizamos el modelo.\n","# Luego hablamos de esto\n","\n","# Entrenamos el modelo\n","mi_RL1.fit(X1_train, Y1_train)\n"],"metadata":{"id":"xRIsorh2FseR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Los pesos del modelo entrenado son los siguientes"],"metadata":{"id":"M4IH7sJ0Tfcf"}},{"cell_type":"code","source":["print(\"El vector w es\")\n","print(mi_RL1.coef_)\n","print(\"Y el sesgo w0 es\")\n","print(mi_RL1.intercept_)"],"metadata":{"id":"S7h3o3tbTgPB"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dibujemos primero la frontera de decisión $P(y=1|\\mathbf{x})=0.5$"],"metadata":{"id":"b3za8f-vTle-"}},{"cell_type":"code","source":["# Estimamos la probabilidad asociada a cada punto con el método .predict_proba\n","probs_LR1 = mi_RL1.predict_proba(grid_norm)\n","\n","fig, ax = plt.subplots()\n","## Dibujamos los datos de entrenamiento\n","plt.plot(X01_train[Y1_train==1,0], X01_train[Y1_train==1,1], 's', label=\"Class 1\")\n","plt.plot(X01_train[Y1_train==0,0], X01_train[Y1_train==0,1], 'ro', label=\"Class 0\")\n","cs = ax.contour(x1, x2, np.reshape(probs_LR1[:,1], np.shape(x1)), [0.5], linestyles='dashed')\n","ax.clabel(cs, inline=1, fontsize=17)\n","plt.xlabel(r'$x_0$')\n","plt.ylabel(r'$x_1$')\n","plt.legend(loc='upper right')\n","plt.title(r'Frontera de decisión RL')\n","# Lineas principales de la rejilla\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.show()"],"metadata":{"id":"8qSYqP5nToJd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Y a continuación el mapa de probabilidades final ..."],"metadata":{"id":"XYsGFWh2Tu0Q"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","## Dibujamos los datos de entrenamiento\n","plt.plot(X01_train[Y1_train==1,0], X01_train[Y1_train==1,1],'s',label=\"Class 1\")\n","plt.plot(X01_train[Y1_train==0,0], X01_train[Y1_train==0,1],'ro',label=\"Class 0\")\n","cs = ax.contourf(x1, x2, np.reshape(probs_LR1[:,1], np.shape(x1)), np.arange(0,1,0.005), linestyles='dashed',cmap='Greys')\n","cbar = fig.colorbar(cs)\n","plt.xlabel(r'$x_0$')\n","plt.ylabel(r'$x_1$')\n","plt.xlim([30, 95])\n","plt.ylim([30, 95])\n","plt.title('Mapa de las probabiliades estimadas por RL')\n","# Lineas principales de la rejilla\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.show()"],"metadata":{"id":"JOrTHsaiTtlY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Antes de seguir, compactemos el código para representar figuras como las que ya hemos visto en una función, con esto ahorraremos muchas líneas de código."],"metadata":{"id":"MMsyUJgbT4nq"}},{"cell_type":"code","source":["def muestra_frontera(X_train,Y_train,x1_grid=None,x2_grid=None,probs_grid=None,dataset=False,frontera=False,thresholds=[0.5],\n","                     prob_levels=False, titulo='Datos',xlabel='$x_0$',ylabel='$x_1$'):\n","    \n","    \"\"\"\n","    - dataset=True --> Representamos solo el dataset\n","    - frontera=True --> Representamos dataset con frontera de decisión (podemos especificar mas niveles con thresholds)\n","    - prob_levels=True --> Representamos dataset con curvas de nivel de probabilidad de clase 1.\n","    \"\"\"\n","    \n","    # Identificamos clases\n","    clases = np.unique(Y_train).astype(np.int32)\n","    labels = ['Class ' + str(int(c)) for c in clases]\n","    \n","    if(dataset==True):\n","        \n","        # Dibujamos únicamente dataset\n","        plt.figure()\n","        for c in clases:\n","            plt.plot(X_train[Y_train==c, 0], X_train[Y_train==c, 1], 's', label=labels[c])\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        plt.legend(loc='upper right')\n","        plt.title(titulo)\n","        plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","        plt.show()\n","    \n","    if(frontera==True):\n","        \n","        # Dibujamos dataset + líneas de contorno definidas en `thresholds`\n","        fig,ax = plt.subplots()\n","        for c in clases:\n","            plt.plot(X_train[Y_train==c, 0], X_train[Y_train==c, 1], 's', label=labels[c])\n","        cs=ax.contour(x1_grid, x2_grid, np.reshape(probs_grid[:, 1], np.shape(x1)), thresholds, linestyles='dashed')\n","        ax.clabel(cs, inline=1, fontsize=12)\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        plt.legend(loc='upper right')\n","        plt.title(titulo)\n","        plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","        plt.show()\n","\n","    if(prob_levels==True):\n","        \n","        # Dibujamos dataset + mapa de probabilidades\n","        fig,ax = plt.subplots()\n","        for c in clases:\n","            plt.plot(X_train[Y_train==c, 0], X_train[Y_train==c, 1], 's', label=labels[c])\n","        cs = ax.contourf(x1_grid, x2_grid, np.reshape(probs_grid[:,1], np.shape(x1)), np.arange(0,1.1,0.0005), linestyles='dashed', cmap='Greys')\n","        cbar = fig.colorbar(cs)\n","        plt.xlabel(xlabel)\n","        plt.ylabel(ylabel)\n","        plt.legend(loc='upper right')\n","        plt.title(titulo)\n","        plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","        plt.show()\n","    "],"metadata":{"id":"F2D5k5gNT9w8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","### 1.6. Ejemplo de una regresión logística no lineal\n","\n","Vamos a considerar un nuevo dataset y repetir el experimento. "],"metadata":{"id":"L-CUEQkGUiWR"}},{"cell_type":"code","source":["data_ejemplo2 = pd.read_csv('http://www.tsc.uc3m.es/~olmos/BBVA/ejemplo2.txt', header=None)\n","\n","data_ejemplo2.head(10)"],"metadata":{"id":"q4jNLle9U70C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Complete el siguiente código, en el que normalizamos el dataset y lo representamos antes y después de normalizar (sólo hay que completar líneas en las que hay presente un `<SOL>`)"],"metadata":{"id":"l2Ab-qIOVBGL"}},{"cell_type":"code","source":["data = np.array(data_ejemplo2)\n","\n","## Dimensiones\n","dims = np.shape(data)\n","N = dims[0]\n","\n","## Separamos X e Y\n","#<SOL> \n","X2=#<SOL>\n","Y2=#<SOL>    \n","#<\\SOL> \n","\n","# Separamos train (80%) de test (20%)\n","#<SOL> \n","X02_train, X02_test, Y2_train, Y2_test = #<SOL>  \n","#<\\SOL> \n","\n","# Variables para la representación de la frontera de decisión (antes de normalizar!)\n","min1 = np.min(X02_train[:,0])\n","max1 = np.max(X02_train[:,0])\n","min2 = np.min(X02_train[:,1])\n","max2 = np.max(X02_train[:,1])\n","\n","print(\"El conjunto de datos de entrenamiento consta {0:d} observaciones de {1:d} dimensiones\\n\".format(X02_train.shape[0], X02_train.shape[1]))\n","\n","# Normalizamos los datos (media 0, varianza 1)\n","#<SOL>\n","transformer2 = #<SOL>   \n","X2_train = #<SOL>    \n","X2_test = #<SOL>  \n","#<\\SOL>\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(X_train=X02_train, Y_train=Y2_train, dataset=True, titulo='Datos entrenamiento')\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(X_train=X2_train, Y_train=Y2_train, dataset=True, titulo='Datos entrenamiento (normalizados)')"],"metadata":{"id":"1c4NxDk2VQJX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Complete el siguiente código, en el que entrenamos un regresor logístico con los hiperparámetros por defecto excepto C=1e8 y representamos la frontera de decisión."],"metadata":{"id":"FkQ0omTSWU9a"}},{"cell_type":"code","source":["# Creamos un objeto del modelo de regresión logística que vamos a utilizar\n","#<SOL>\n","mi_RL2 = \n","#</SOL>\n","\n","# Entrenamos mi_RL2, el regresor logístico anterior, usando los datos normalizados\n","#<SOL>\n","\n","#</SOL>\n","\n","# Obtenemos una rejilla, grid, de puntos en los que evaluaremos nuestro RL\n","x1,x2 = np.mgrid[min1:max1:(max1-min1)/50, min2:max2:(max2-min2)/50]\n","grid = np.transpose(np.row_stack([x1.ravel(), x2.ravel()]))\n","\n","# Normalizamos la rejilla\n","#<SOL>\n","grid_norm =    \n","#</SOL>\n","\n","# Estimamos la probabilidad asociada a cada punto con el método .predic_proba\n","#<SOL>\n","probs_LR2 = \n","#</SOL>\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR2, X_train=X02_train, Y_train=Y2_train, frontera=True, titulo='Frontera de decisión RL')\n","\n"],"metadata":{"id":"vAhGgNuZWTK8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obviamente, en este dataset no podemos esperar que una separación líneal funcione correctamente. De hecho, podemos calcular el número de etiquetas correctamente clasificadas tanto en train como test del siguiente modo:"],"metadata":{"id":"Cum2DtT0W68d"}},{"cell_type":"code","source":["accuracy_train = mi_RL2.score(X2_train, Y2_train)\n","accuracy_test = mi_RL2.score(X2_test, Y2_test)\n","\n","print(\"Accuracy train {0:f}%. Accuracy test {1:f}%\\n\".format(accuracy_train*100, accuracy_test*100))\n"],"metadata":{"id":"CzBwyPLdW_Uz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una transformación de las variables de entrada nos permitirá generalizar la frontera de decisión. En particular, vamos a incluir transformaciones polinómicas tal y como hicisteis en el caso de regresión:\n","\n","$$\\mathbf{x} = [x_1,x_2] ~~ \\Rightarrow \\phi(\\mathbf{x})= [x_1,x_2,x_1^2,x_2^2,x_1x_2]$$\n","\n","de tal manera que ahora entrenaremos un RL del tipo\n","\n","$$P(Y=1| {\\bf x}) =  \\frac{1}{1+e^{-({\\bf w}^T {\\phi(\\mathbf{x})}+w_0)}}=\\sigma({\\bf w}^T {\\phi(\\mathbf{x})}+w_0)$$\n","\n","Y la frontera de decisión viene dada por la ecuación\n","\n","$$ {\\bf w}^T {\\phi(\\mathbf{x})}+w_0 = 0$$\n","\n","que en este caso **corresponde a la ecuación de una elipse** (en el espacio $(x_1,x_2)$)."],"metadata":{"id":"5eRXpckGXE__"}},{"cell_type":"code","source":["poly_grado2 = PolynomialFeatures(2,include_bias=False) #No usamos x^0 = 1"],"metadata":{"id":"G3SGm_KKXTBh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Añadimos las transformaciones en el espacio original de los datos\n","\n","X02_train_grado2 = poly_grado2.fit_transform(X02_train)\n","\n","X02_test_grado2 = poly_grado2.transform(X02_test)\n","\n","print(\"El conjunto de datos de entrenamiento consta {0:d} observaciones de {1:d} dimensiones\\n\".format(X02_train_grado2.shape[0], X02_train_grado2.shape[1]))\n"],"metadata":{"id":"eFOb2vr2XYWW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Complete el siguiente código, en el que normalizamos `X02_train_grado2` y `X02_test_grado2`, entrenamos un RL con los parámetros por defecto (C=1e8) y representamos la frontera de decisión."],"metadata":{"id":"cfETBXE0Xgvv"}},{"cell_type":"code","source":["# Normalizamos los datos (media 0, varianza 1)\n","#<SOL>\n","transformer_grado2 = #<SOL>    \n","X2_train_grado2 = #<SOL>    \n","X2_test_grado2 = #<SOL>   \n","#</SOL>\n","\n","# Entrenamos el regresor logístico\n","#<SOL>\n","mi_RL2_grado2 = #<SOL>  \n","\n","#</SOL>\n","\n","#Obtenemos una rejilla de puntos en los que evaluaremos nuestro RL (espacio original de los datos!)\n","x1,x2 = np.mgrid[min1:max1:(max1-min1)/50, min2:max2:(max2-min2)/50]\n","grid = np.transpose(np.row_stack([x1.ravel(), x2.ravel()]))\n","# Añadimos las características polinómicas a la rejilla \n","#<SOL>\n","grid_ext = #<SOL>\n","#</SOL>\n","\n","# Normalizamos la rejilla\n","#<SOL>\n","grid_norm = #<SOL>   \n","#</SOL>\n","\n","#Estimamos la probabilidad asociada a cada punto con el método .predic_proba\n","#<SOL>\n","probs_LR_grado2 = #<SOL>\n","#</SOL>\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado2, X_train=X02_train_grado2, Y_train=Y2_train,\n","                 frontera=True, titulo=r'Frontera de decisión RL cuadrático')\n","\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado2, X_train=X02_train_grado2, Y_train=Y2_train,\n","                 prob_levels=True, titulo=r'Probabilidades RL cuadrático')"],"metadata":{"id":"yHF0TcYmXnbc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Obviamente el resultado es mucho mejor. Imprimamos el porcentaje de etiquetas acertadas ..."],"metadata":{"id":"S6xwoYDUYJwQ"}},{"cell_type":"code","source":["accuracy_train = mi_RL2_grado2.score(X2_train_grado2,Y2_train)\n","accuracy_test = mi_RL2_grado2.score(X2_test_grado2, Y2_test)\n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))"],"metadata":{"id":"PosjbVUfYMyi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### 1.6.1 Visualización del sobreajuste\n","\n","Comprobemos ahora qué ocurre si añadimos extensiones polinómicas hasta orden 20. El siguiente código (muy parecido al anterior) realiza dicho experimento ..."],"metadata":{"id":"Q_HDqapZYai9"}},{"cell_type":"code","source":["poly_grado20 = PolynomialFeatures(20, include_bias=False) #No usamos x^0 = 1\n","\n","X02_train_grado20 = poly_grado20.fit_transform(X02_train)\n","\n","X02_test_grado20 = poly_grado20.transform(X02_test)\n","\n","print(\"El conjunto de datos de entrenamiento consta {0:d} observaciones de {1:d} dimensiones\\n\".format(X02_train_grado20.shape[0], X02_train_grado20.shape[1]))\n"],"metadata":{"id":"GGUItB3lYlKo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Complete el siguiente código, en el que normalizamos `X02_train_grado20` y `X02_test_grado20`, entrenamos un RL con los hiperparámetros por defecto, excepto C igual a 1e8, max_iter igual a 10000 y solver='liblinear'. Luego, representamos la frontera de decisión."],"metadata":{"id":"P2Ph9bWwYtIy"}},{"cell_type":"code","source":["# Normalizamos los datos (media 0, varianza 1)\n","#<SOL>\n","transformer_grado20 = #<SOL>  \n","X2_train_grado20 = #<SOL> \n","X2_test_grado20 = #<SOL>  \n","#</SOL>\n","\n","# Entrenamos el regresor logístico\n","mi_RL2_grado20 = LogisticRegression(C=1e8, max_iter=10000, solver='liblinear')    \n","#<SOL>\n","\n","#</SOL>\n","\n","#Obtenemos una rejilla de puntos en los que evaluaremos nuestro RL (espacio original de los datos!)\n","grid_ext = poly_grado20.transform(grid)\n","\n","# Normalizamos la rejilla\n","#<SOL>\n","grid_norm = #<SOL>\n","#</SOL>\n","\n","\n","#Estimamos la probabilidad asociada a cada punto con el método .predic_proba\n","#<SOL>\n","probs_LR_grado20 = #<SOL>\n","#</SOL>\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado20, X_train=X02_train_grado20, Y_train=Y2_train,\n","                 frontera=True, titulo='Frontera de decisión RL Grado 20')\n","\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado20, X_train=X02_train_grado20, Y_train=Y2_train,\n","                 prob_levels=True, titulo='Probabilidades RL Grado 20')"],"metadata":{"id":"Cwkhn23dYrpY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos apreciar claramente una **predición mucho mas sobreajustada y sensible a la distribución** de los puntos de entrenamiento. El efecto del sobreajuste es evidente si comparamos la fracción de etiquetas correctamente detectada en el conjunto de training y de test."],"metadata":{"id":"_sWj3op1ZIOb"}},{"cell_type":"code","source":["accuracy_train = mi_RL2_grado20.score(X2_train_grado20,Y2_train)\n","accuracy_test = mi_RL2_grado20.score(X2_test_grado20,Y2_test)\n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))"],"metadata":{"id":"rp2HON49ZOj1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como podemos ver, ahora en el conjunto de entrenamiento hemos mejorado sensiblemente, pero no así en el de test. Si representamos la frontera de decisión frente al conjunto de test podemos  apreciar muchos puntos mal clasificados."],"metadata":{"id":"opeeVfuRZU8B"}},{"cell_type":"code","source":["muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado20, X_train=X02_test_grado20, Y_train=Y2_test,\n","                 frontera=True, titulo='Frontera de decisión RL Grado 20, Datos de Test')"],"metadata":{"id":"T4R9RdftZXrS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una forma alternativa de detectar que un modelo paramétrico como RL está sobreajustado es mirar la magnitud de los pesos (igual que en el caso de regresión lineal), donde podemos comprobar que cuanto más sobreajuste, más valores extremos obtenemos."],"metadata":{"id":"PwF_ybElZyeZ"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","plt.stem(mi_RL2_grado2.coef_.T,use_line_collection=True)\n","plt.xlabel('$j$')\n","plt.ylabel('Valor peso $w_j$')\n","plt.title('Pesos RL polinomio grado 2')\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.text(5, 0, r'Valor absoluto medio $\\overline{|w|}$='+'{0:.2f}'.format(np.mean(np.abs(mi_RL2_grado2.coef_))), fontsize=15)\n","plt.show()\n","\n","\n","\n","fig, ax = plt.subplots()\n","plt.stem(mi_RL2_grado20.coef_.T,use_line_collection=True)\n","plt.xlabel('$j$')\n","plt.ylabel('Valor peso $w_j$')\n","plt.title('Pesos RL polinomio grado 20')\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","ax.text(290, 0, r'Valor absoluto medio $\\overline{|w|}$='+'{0:.2f}'.format(np.mean(np.abs(mi_RL2_grado20.coef_))), fontsize=15)\n","plt.show()"],"metadata":{"id":"sdSC2ojCZyDS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### 1.7 Regularización en Regresión Logística\n","---\n","\n","Podemos penalizar la magnitud de los pesos añadiendo un término de **regularización** en la función de coste del RL:\n","\n","\n","* Regularización cuadrática (L2):\n","\n","$$ \\bf w^* = \\displaystyle \\underset{{\\bf w}}{\\operatorname{min}} l({\\bf w}) + \\frac{1}{C} \\Vert {\\bf w} \\Vert_2^2$$\n","\n","* Regularización absoluta (L1):\n","\n","$$ \\bf w^* = \\displaystyle \\underset{{\\bf w}}{\\operatorname{min}} l({\\bf w}) + \\frac{1}{C} \\Vert {\\bf w} \\Vert_1$$\n","\n","donde $l({\\bf w})$ es la función de coste de RL que vimos anteriormente. Tal y como visteis en el caso de regresión, L2 consigue una regularización más uniforme a lo largo de todos los pesos (y algo mejor de precisión de clasificación ya que tiene más grados de libertad), mientras que con L1 la solución tiende a ser *sparse*, muchos pesos se van a cero y esto nos permite determinar qué características de entrada son más relevantes para el modelo.\n","\n","La [implementación de RL en sklearn](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) nos permite seleccionar de forma sencilla el tipo de regularización y el parámetro $C$.\n"],"metadata":{"id":"JOlf5unoaBEI"}},{"cell_type":"markdown","source":["Para el último ejemplo (expansión polinómica de grado 20), vamos a validar $C$ utilizando [GridSearchCV](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) y analizar el resultado. "],"metadata":{"id":"W_2IULyiaX_I"}},{"cell_type":"code","source":["from sklearn.model_selection import GridSearchCV\n","\n","rango_C = np.logspace(-3, 3, 10)  # Rango C en escala logarítmica (base 10). Esto es, 20 puntos desde 10^3, a 10^3.\n","diccionario_parametros = #</SOL>\n","nfold = 10 # Número de particiones train/validación\n","\n","\"\"\" Ajusto C por validación cruzada\n","El optimizador por defecto ('lbfgs') no acepta regularización l1. \n","Usamos 'liblinear' siguiendo las recomendaciones de la librería.\n","\"\"\"\n","mi_LR2_grado20_CV  = GridSearchCV(estimator=LogisticRegression(penalty='l1', max_iter=10000, solver='liblinear'),\n","                                  param_grid=diccionario_parametros, cv=nfold)\n","# Entrenar el modelo\n","#<SOL>\n","\n","#</SOL>\n","\n","print(\"El mejor parámetro C es {0:.2f}\".format(mi_LR2_grado20_CV.best_params_['C']))\n","                        \n","# Score de clasificación en train/test\n","#<SOL>\n","accuracy_train = #<SOL> \n","accuracy_test = #<SOL>\n","#</SOL>\n","\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train*100, accuracy_test*100))\n"],"metadata":{"id":"ygYPWiTzacs3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Dibujemos los pesos y la correspondiente región de decisión"],"metadata":{"id":"ltyQoTRja0fS"}},{"cell_type":"code","source":["fig, ax = plt.subplots()\n","\n","plt.stem(mi_LR2_grado20_CV.best_estimator_.coef_.T, use_line_collection=True)\n","plt.xlabel('Índice peso $j$')\n","plt.ylabel('Valor peso $w_j$')\n","plt.title('Pesos RL polinomio grado 20. Regularización L1')\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.show()\n","\n"],"metadata":{"id":"Wz7sBdYEa3QV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Estimamos la probabilidad asociada a cada punto con el método .predic_proba\n","probs_LR_grado20_CV = mi_LR2_grado20_CV.best_estimator_.predict_proba(grid_norm)\n","\n","# Representaciones usando la función muestra_frontera\n","muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado20_CV, X_train=X02_train_grado20, Y_train=Y2_train,\n","                 frontera=True, titulo=r'Frontera de decisión RL Grado 20 con Reg. L1')"],"metadata":{"id":"AqGAYExAa66M"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Podemos comprobar cómo sin ser exactamente una elipse, la solución es muy parecida a la obtenida con una expansión polinómica de grado dos."],"metadata":{"id":"Fna0TY38bBfO"}},{"cell_type":"markdown","source":["### 1.8. Medida de prestaciones en clasificación binaria\n","---\n","\n","- Hasta ahora hemos comparado los distintos clasificadores en base a una métrica, la fracción de etiquetas de train/test correctamente detectadas (*accuracy* en inglés). \n","$$$$\n","- Sin embargo, **no todos los errores son equiparables**:\n","    - Por ejemplo en un problema de detectar correo spam,  etiquetar un correo válido como spam es un error que podemos considerar más grave que dejar de etiquetar un correo no deseado como spam.\n","    - En un problema de decisión de concesión de un préstamo, conceder un préstamo a una persona no solvente implica unas pérdidas mayores que dejar pasar a un cliente solvente.\n","    \n","En este sentido, utilizar únicamente la fracción de etiquetas de train/test correctamente detectadas puede no ser la mejor idea. Vamos a definir una serie de métricas que proporcionan un espectro más completo para caracterizar un clasificador."],"metadata":{"id":"9D-Wg9hacsGw"}},{"cell_type":"markdown","source":["En un problema binario, cuando el clasificador realiza una estimación para un nuevo dato podemos diferenciar **cuatro** posibles eventos:\n","\n","| | **Predicción** $D=1$ | **Predicción** $D=0$ |\n","| --- | --- | --- |\n","| **Etiqueta real** $Y=1$  | True Positive (TP) | False Negative or Missing (FN)|\n","| **Etiqueta real** $Y=0$  |  False Positive or False Alarm (FP)  | True Negative (TN) | \n","\n","A partir de estos eventos medidos sobre un conjunto de datos clasificados podemos definir las siguientes métricas:\n","* **Tasa de falsos positivos (o de falsa alarma)** es la razón entre el número de falsos positivos y el número total de datos con etiqueta real $Y=0$ (la fracción de negativos que se nos han colado como positivos):\n","\n","$$ FPR = \\frac{\\# FP}{ \\# TN + \\# FP }$$\n","\n","* **Especificidad** o *true negative rate* es la razón entre el número de etiquetas negativas correctamente detectadas y el total de datos con etiqueta negativa $Y=0$ (qué fracción de negativos he conseguido detectar del total de negativos):\n","\n","$$ E = \\frac{\\# TN}{\\# TN + \\# FP} = 1- FPR$$\n","\n","\n","* **Tasa de falsos negativos (o de pérdidas)** es la razón entre el número falsos negativos y el total de datos con etiqueta positiva $Y=1$ (la fracción de positivos que se nos han colado como negativos):\n","\n","$$ FNR = \\frac{\\# FN}{ \\# TP + \\# FN }$$\n","\n","\n","* **Sensibilidad** o *recall* o *true positive rate* es la razón entre el número de etiquetas positivas correctamente detectadas y el total de datos con etiqueta positiva $Y=1$ (qué fracción de positivos he conseguido detectar del total de positivos):\n","\n","$$ R = \\frac{\\# TP}{\\# TP + \\# FN} = 1- FNR$$\n","\n","\n","* **Precisión** es la razón entre el número de etiquetas positivas correctamente detectadas y el número total de datos **clasificados** con etiqueta positiva (qué fracción de los que yo he detectado como $Y=1$ realmente lo son):\n","\n","$$ P = \\frac{\\# TP}{\\# TP + \\# FP}$$\n","\n","La siguiente figura ayuda a entender las anteriores métricas [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)\n","\n","<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"30%\"> \n","\n","La precisión (P) y el *recall* (R) suelen combinarse en una única métrica denominada **F-score** o **F1-score**:\n","\n","$$F_{\\text{score}} = 2 \\frac{P\\cdot R}{P+R}$$\n","\n","Esta métrica varía entre 0 (peor caso) y 1 (mejor caso)."],"metadata":{"id":"Xn51fctGomJi"}},{"cell_type":"markdown","source":["#### Regresión Logistica L2 en dataset de Spam\n","\n","En este apartado, estudiaremos diferentes métricas de clasificación en un problema más real, con más muestras y variables, sobre detección de correo spam.\n","\n","#### Spam detection database\n","- 4601 observaciones (1813 spam, 39.4%)\n","- target binario: spam ($y=1$) o no spam ($y=0$)\n","- No disponemos de los textos, las observaciones están formadas por 57 variables continuas\n","  - 48 variables reales continuas en el intervalo [0,100] *word_freq_WORD*: frecuencia de aparición de la palabra *WORD* en el correo (en porcentaje)\n","  - 6 variables reales continuas en el intervalo [0,100] *char_freq_CHAR*: frecuencia de aparición del caracter *CHAR* en el correo (porcentaje)\n","  - 1 variable real continua: longitud promedio de secuencias de caracteres en mayúscula ininterrumpidos\n","  - 1 variable entera continua: longitud de la secuencia de mayúsculas ininterrumpida más larga\n","  - 1 variable entera continua: número total de mayúsculas en el correo\n","  "],"metadata":{"id":"uPK9koLDts88"}},{"cell_type":"code","source":["def load_spam():\n","    data = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/spambase/spambase.data',header=None)\n","    data.columns=[\"wf_make\",         \n","        \"wf_address\",      \n","        \"wf_all\",          \n","        \"wf_3d\",           \n","        \"wf_our\",          \n","        \"wf_over\",         \n","        \"wf_remove\",       \n","        \"wf_internet\",     \n","        \"wf_order\",        \n","        \"wf_mail\",         \n","        \"wf_receive\",      \n","        \"wf_will\",         \n","        \"wf_people\",       \n","        \"wf_report\",       \n","        \"wf_addresses\",    \n","        \"wf_free\",         \n","        \"wf_business\",     \n","        \"wf_email\",        \n","        \"wf_you\",          \n","        \"wf_credit\",       \n","        \"wf_your\",         \n","        \"wf_font\",         \n","        \"wf_000\",          \n","        \"wf_money\",        \n","        \"wf_hp\",           \n","        \"wf_hpl\",          \n","        \"wf_george\",       \n","        \"wf_650\",          \n","        \"wf_lab\",          \n","        \"wf_labs\",         \n","        \"wf_telnet\",       \n","        \"wf_857\",          \n","        \"wf_data\",         \n","        \"wf_415\",          \n","        \"wf_85\",           \n","        \"wf_technology\",   \n","        \"wf_1999\",         \n","        \"wf_parts\",        \n","        \"wf_pm\",           \n","        \"wf_direct\",       \n","        \"wf_cs\",           \n","        \"wf_meeting\",      \n","        \"wf_original\",     \n","        \"wf_project\",      \n","        \"wf_re\",           \n","        \"wf_edu\",          \n","        \"wf_table\",        \n","        \"wf_conference\",   \n","        \"cf_;\",            \n","        \"cf_(\",            \n","        \"cf_[\",            \n","        \"cf_!\",            \n","        \"cf_$\",            \n","        \"cf_#\",            \n","        \"cap_average\", \n","        \"cap_longest\", \n","        \"cap_total\",\n","        \"target\"]\n","    return data"],"metadata":{"id":"75-U9kO8urAd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = load_spam()\n","X0_spam = data[data.columns[:57]].values\n","Y_spam = data['target'].values\n","\n","print(\"Cargadas {0:d} observaciones con {1:d} columnas\\n\".format(len(data), len(data.columns)))\n","print(\"Ejemplos\")\n","data.head()"],"metadata":{"id":"xoJzdmq3uhDR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Divida la base de datos en train (70%) y test (30%). Normalice la base de datos de acuerdo a los estadísticos del conjunto de entrenamiento"],"metadata":{"id":"vk7Q0WqEu0Pm"}},{"cell_type":"code","source":["# Dividimos train/test. Normalizamos\n","#<SOL>\n","X0_spam_train, X0_spam_test, Y_spam_train, Y_spam_test = #<SOL>\n","#</SOL>\n","\n","# X0--> Datos originales, X --> Normalizados\n","#<SOL>\n","transformer_spam = #<SOL>   \n","X_spam_train = #<SOL>\n","X_spam_test = #<SOL> \n","#</SOL>\n"],"metadata":{"id":"9X6wik-tu2gi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["> **Ejercicio:** Complete el siguiente código. En el que se entrena un RL con regularización L2"],"metadata":{"id":"a_zXM0Y3ujAq"}},{"cell_type":"code","source":["# Entrenamiento RL con validación parámetro de regularización L2\n","rango_C = np.logspace(-2, 1, 20)  # Rango C en escala logarítmica (base 10). Esto es, 20 puntos desde 10^-2, a 10^1.\n","#<SOL>\n","diccionario_parametros = #<SOL>\n","#<S/OL>\n","nfold = 10 # Número de particiones train/validación\n","\n","#<SOL>\n","RL_spam_L2 =  #<SOL>\n","#</SOL>\n","\n","#Entrenamiento \n","#<SOL>\n","\n","#</SOL>\n","\n","#Cálculo del score\n","#<SOL>\n","accuracy_train_rl_l2 = #<SOL>\n","accuracy_test_rl_l2 = #<SOL> \n","#</SOL>\n","\n","print(\"El parámetro de regularización seleccionado es C={0:.2f}\".format(RL_spam_L2.best_params_['C']))\n","print(\"Accuracy train {0:.2f}%. Accuracy test {1:.2f}%\\n\".format(accuracy_train_rl_l2*100, accuracy_test_rl_l2*100))"],"metadata":{"id":"hn5xVegitfLn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Vamos a calcular algunas de estas métricas para el clasificador entrenado anteriormente. Usaremos las siguientes funciones directamente proporcionadas por [sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html):\n","\n","- [Precision score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_score.html)\n","- [Recall score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.recall_score.html)\n","- [F1 score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html#sklearn.metrics.f1_score)"],"metadata":{"id":"W0Tot3rwvhNQ"}},{"cell_type":"code","source":["from sklearn.metrics import f1_score, recall_score, precision_score\n","\n","# Métricas para el RL con regularización L2\n","\n","precision_RL = precision_score(Y_spam_test, RL_spam_L2.predict(X_spam_test))\n","recall_RL = recall_score(Y_spam_test, RL_spam_L2.predict(X_spam_test))\n","F1_RL = f1_score(Y_spam_test, RL_spam_L2.predict(X_spam_test))\n","\n","print(\"\\n Precision P = {0:.2f}\".format(precision_RL))\n","print(\"\\n Recall R = {0:.2f}\".format(recall_RL))\n","print(\"\\n F1 score, F1 = {0:.2f}\".format(F1_RL))"],"metadata":{"id":"ttWlclc4qoht"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Variando el umbral de detección\n","\n","Hasta ahora, hemos supuesto que los distintos clasificadores deciden que la etiqueta asociada a un dato es aquella para la cual la probabilidad estimada es mayor que 0.5. Pero este valor, conocido como **umbral de decisión** es un hiperparámetro más que tenemos que fijar de acuerdo a nuestros requisitos de Precision, Recall, FPR o FNR.\n","\n","Por ejemplo, si queremos maximizar el Recall en general tomaremos un umbral para detección de clase positiva por debajo de 0.5 (por ejemplo 0.2). \n","\n","- Esto incrementará la tasa de datos detectados como positivos y será mas dificil que se nos haya escapado alguno. \n","\n","- Pero también incrementará la tasa de falsos positivos y hará bajar la precisión. Esto es, las métricas típicamente están comprometidas unas con otras. "],"metadata":{"id":"ToJTkrJdq2q9"}},{"cell_type":"markdown","source":["Ilustremos estos compromisos entre métricas en el ejemplo 2D del comienzo del Notebook"],"metadata":{"id":"OqEVK9rowWBW"}},{"cell_type":"code","source":["muestra_frontera(x1_grid=x1, x2_grid=x2, probs_grid=probs_LR_grado2, X_train=X02_train_grado2, Y_train=Y2_train,\n","                 frontera=True, thresholds=[0.1,0.5,0.9], titulo=r'RL expansión grado 2. Umbrales en 0.1, 0.5 y 0.9')"],"metadata":{"id":"IXmOkZGPwYvP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Calculemos FPR y Recall para los tres umbrales\n","\n","umbrales = [0.9, 0.5, 0.1]\n","\n","R = []\n","\n","FPR = []\n","\n","for u in umbrales:\n","    \n","    # Implementamos las fórmulas de las métricas directamente, para mostrar \n","    # que son sencillas (se pueden usar las funciones de sklearn)\n","\n","    Y_pred = (mi_RL2_grado2.predict_proba(X2_train_grado2)[:,1]>=u)\n","    \n","    FP = np.sum((Y_pred==1) & (Y2_train==0))\n","    \n","    FN = np.sum((Y_pred==0) & (Y2_train==1))\n","    \n","    TP = np.sum((Y_pred==1) & (Y2_train==1))\n","    \n","    TN = np.sum((Y_pred==0) & (Y2_train==0))\n","    \n","    R.append(TP/(TP+FN))\n","    \n","    FPR.append(FP/(FP+TN))\n","    \n","    print('Umbral en {0:.2f}. Recall {1:.2f}. FPR {2:.2f}'.format(u,R[-1], FPR[-1]))\n"],"metadata":{"id":"FWGhfIiYwh5V"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Para hacernos una idea global del compromiso entre las distintas métricas es habitual analizar la evolución del recall (R) frente a la tasa de falsos positivos (FPR) o bien frente a la precisión (P) a medida que variamos el umbral de decisión.\n","\n","- En la curva **ROC** (receiver operating characteristic) mostramos la evolución del Recall frente a FPR.\n","- En la curva **Precision-Recall** mostramos la evolución de la precisión P frente al Recall.\n","\n","En ambos casos, el comportamiento global se mide utilizando el **área bajo la curva** (AUC). Idealmente, es igual a 1.\n","\n","Dibujemos estas curvas para el ejemplo anterior. Usando las funciones [roc_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) y [precision_recall_curve](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_curve.html#sklearn.metrics.precision_recall_curve) de sklearn. Para el área bajo la curva usamos [roc_auc_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) y [average_precision_score](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score) respectivamente."],"metadata":{"id":"16ZCMzfewt5E"}},{"cell_type":"code","source":["from sklearn import metrics\n","\n","# ROC curve\n","\n","fpr, recall, thresholds = metrics.roc_curve(Y2_train, mi_RL2_grado2.predict_proba(X2_train_grado2)[:,1], pos_label=1)\n","\n","fig,ax = plt.subplots()\n","plt.plot(fpr,recall,lw=2.5,label='Curva ROC')\n","for ii,u in enumerate(umbrales):\n","    plt.plot(FPR[ii], R[ii],'*', ms=15, label='Umbral en '+str(u))\n","plt.legend(loc=7)\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('Recall (R)')\n","plt.title('Curva ROC. RL con L2.')\n","\n","# Area bajo la curva ROC\n","\n","area_roc = metrics.roc_auc_score(Y2_train, mi_RL2_grado2.predict_proba(X2_train_grado2)[:,1])\n","\n","ax.text(1.08, 0.98, r'Umbral igual a 0.0 (todos positivos)')\n","\n","ax.text(0.025, 0.05, r'Umbral igual a 1.0 (todos negativos)')\n","\n","plt.show()\n"],"metadata":{"id":"BarVKGrNxXpX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Curvas ROC y PR en Spam\n","\n","Estudiemos las curvas ROC y PR en la base de datos Spam con el regresor lógistico con regularización L2"],"metadata":{"id":"IIZ-a9NXzFZR"}},{"cell_type":"code","source":["# ROC curve\n","\n","fpr, recall, thresholds = metrics.roc_curve(Y_spam_test, RL_spam_L2.predict_proba(X_spam_test)[:,1], pos_label=1)\n","\n","fig,ax = plt.subplots()\n","plt.plot(fpr, recall, lw=2.5)\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('False Positive Rate')\n","plt.ylabel('Recall (R)')\n","plt.title('Curva ROC. Spam dataset. RL con L2.')\n","\n","# Area bajo la curva ROC\n","\n","area_roc = metrics.roc_auc_score(Y_spam_test, RL_spam_L2.predict_proba(X_spam_test)[:,1])\n","\n","ax.text(1.08, 0.98, 'Umbral igual a 0.0 (todos positivos)')\n","\n","ax.text(0.025, 0.05, 'Umbral igual a 1.0 (todos negativos)')\n","\n","ax.text(0.3, 0.5, \"El área bajo la curva ROC es {0:.2f}\".format(area_roc), style='italic',\n","        bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n","\n","plt.show()\n","\n","\n","# Precision-Recall Curve\n","\n","precision, recall, thresholds = metrics.precision_recall_curve(Y_spam_test, RL_spam_L2.predict_proba(X_spam_test)[:,1], pos_label=1)\n","\n","\n","fig,ax = plt.subplots()\n","plt.plot(recall, precision,lw=2.5)\n","plt.grid(visible=True, which='major', color='gray', alpha=0.6, linestyle='dotted', lw=1.5)\n","plt.xlabel('Recall (R)')\n","plt.ylabel('Precision (P)')\n","plt.title('Curva P-R. Spam dataset. RL con L2.')\n","\n","\n","# Area bajo la curva ROC\n","\n","area_roc = metrics.average_precision_score(Y_spam_test, RL_spam_L2.predict_proba(X_spam_test)[:,1])\n","\n","ax.text(0.01, 0.93, 'Umbral igual a 1.0 (todos negativos)')\n","\n","ax.text(1.08, 0.55, 'Umbral igual a 0.0 (todos positivos)')\n","\n","ax.text(0.3, 0.7, \"El área bajo la curva PR es {0:.2f}\".format(area_roc), style='italic',\n","        bbox={'facecolor': 'red', 'alpha': 0.5, 'pad': 10})\n","\n","plt.show()"],"metadata":{"id":"M1djqGlizMHp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una vez que hemos escogido el punto de trabajo en la curva ROC o PR, es interesante dibujar ambas curvas ya en función del umbral:"],"metadata":{"id":"Hd_29372zagg"}},{"cell_type":"code","source":["plt.plot(thresholds, precision[:-1], 'b', label='Precision')\n","plt.plot(thresholds, recall[:-1], 'r', label='Recall')\n","plt.xlabel('Umbral de decisión')\n","plt.grid()\n","plt.legend()"],"metadata":{"id":"I-LOc1qHzbNz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Matriz de confusión\n","\n","Otra de las formas de evaluar un modelo de clasificación es a través de la matriz de confusión. Esta nos permite ver la cantidad de muestras clasificadas bien y mal de cada clase, o incluso, en un problema multi-clase ver si hay dos clases que se estén confundiendo."],"metadata":{"id":"TszpdfrIZy11"}},{"cell_type":"code","source":["from sklearn.metrics import ConfusionMatrixDisplay\n","\n","ConfusionMatrixDisplay.from_estimator(RL_spam_L2, X_spam_test, Y_spam_test)"],"metadata":{"id":"_Xl6frI_Y5cM"},"execution_count":null,"outputs":[]}]}